{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker PySpark XGBoost Regression Example\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Setup](#Setup)\n",
    "3. [Data Cleansing](#Data-Cleansing)\n",
    "4. [Feature Trend Analysis](#Feature-Trend-Analysis)\n",
    "5. [Feature Enginnering](#Feature-Engineering)\n",
    "6. [Split data into training and test dataset](#Split-data-into-training-and-test-dataset)\n",
    "7. [Training and Hosting XGBoost Model](#Training-and-Hosting-XGBoost-Model)\n",
    "8. [Run Predictions](#Run-Predictions)\n",
    "9. [Clean up](#Clean-up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This notebook will show how to perfrom Tips prediction using XGBoost algorithm on Amazon SageMaker through the SageMaker PySpark library. We will train on Amazon SageMaker using XGBoost on curated NYC Taxi dataset, host the trained model on Amazon SageMaker, and then make predictions against that hosted model.\n",
    "\n",
    "Unlike the other notebooks that demonstrate XGBoost on Amazon SageMaker, this notebook uses a SparkSession to manipulate data, and uses the SageMaker Spark library to interact with SageMaker with Spark Estimators and Transformers.\n",
    "\n",
    "You can visit SageMaker Spark's GitHub repository at https://github.com/aws/sagemaker-spark to learn more about SageMaker Spark.\n",
    "\n",
    "You can visit XGBoost's GitHub repository at https://github.com/dmlc/xgboost to learn more about XGBoost\n",
    "\n",
    "This notebook was created and tested on an ml.m4.xlarge notebook instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "1. Import Spark and Glue packages\n",
    "2. Initialize GlueContext and SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "import sagemaker_pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, OneHotEncoder, VectorAssembler, IndexToString\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "sc=sc if 'sc' in vars() else SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Current IAM Execution Role "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "roleName = 'AWSGlueServiceSageMakerNotebookRole-nyctaxi'\n",
    "\n",
    "iam = boto3.client('iam')\n",
    "role = iam.get_role(RoleName=roleName)\n",
    "execution_role = role[\"Role\"]['Arn']\n",
    "print('IAM role arn: {}'.format(execution_role))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "Read Glue Data catalog for yello taxi optimized Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyctaxidyf = glueContext.create_dynamic_frame.from_catalog(database='nyctaxi',table_name='yellow_opt'\\\n",
    "                                                          ,push_down_predicate='pu_year=2017 and pu_month=1')\n",
    "\n",
    "nyctaxidyf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Glue Dynamic Frame to Spark Dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyctaxidf = nyctaxidyf.toDF().limit(100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleansing\n",
    "\n",
    "1. Pickup and dropoff timestamps are not useful directly in training but using 'Day of month' and 'Day of week' are useful features\n",
    "3. Remove samples with zero or negative total amount\n",
    "4. Remove samples with negative tip amount\n",
    "5. Remove all non-electronic transactions as most drivers do not report tips on cash transactions (payment_type = 2)\n",
    "6. Remove all payments of type 'Dispute','No Charge','Unknown' (payment_type = 4 or 3, 5 )\n",
    "6. Removed samples where tip was more than 100% of the fare amount as those are outliers and have a significant impact on algorithms which try to optimize MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil import parser\n",
    "from pyspark.sql.types import IntegerType,StringType,ArrayType\n",
    "\n",
    "\n",
    "nyctaxidf1 = nyctaxidf.withColumn('pickup_dow_str',date_format(col('pu_datetime'),'E'))\\\n",
    "    .withColumn('pickup_hr',hour(col('pu_datetime')))\\\n",
    "    .withColumn('dropoff_dow_str',date_format(col('do_datetime'),'E'))\\\n",
    "    .withColumn('dropoff_hr',hour(col('do_datetime')))\\\n",
    "    .withColumn('taxes',col('extra')+col('mta_tax')+col('tolls_amount')+col('improvement_surcharge'))\\\n",
    "    .filter( (nyctaxidf.total_amount > 0) & (nyctaxidf.fare_amount > 0))\\\n",
    "    .filter(nyctaxidf.payment_type == 1)\\\n",
    "    .filter(nyctaxidf.fare_amount > nyctaxidf.tip_amount)\\\n",
    "    .filter(nyctaxidf.tip_amount >= 0)\\\n",
    "    .dropna() \n",
    "\n",
    "print(\"Cleansed Dataset sample count:{}\".format(nyctaxidf1.count()))\n",
    "\n",
    "nyctaxidf2 = nyctaxidf1.select('tip_amount','pickup_dow_str','pickup_hr','pu_locationid',\\\n",
    "                               'dropoff_dow_str','dropoff_hr','do_locationid',\\\n",
    "                               'trip_distance','total_amount')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Trend Analysis\n",
    "\n",
    "In this section we will observe trends in feature. This phase allows to remove any outlier samples that may skew training and predictions. This is very important aspect of Data science. We will be only looking at Tip_ratio distribution but other features(e.g. Geographical featues - Taxi_Zones, Temporal Features - Pickup/Drop off datetimes, Trip features - Trip distance, Number of passengers per ride) are also influential. Due to lack of time you can observe these features as homework\n",
    "\n",
    "Register a Spark Temp View using Spark Dataframe\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-programming-guide.html#running-sql-queries-programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.dropTempView(\"nytaxi_view\")\n",
    "nyctaxidf2.createTempView(\"nytaxi_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql -q -o tip_ratio_pd\n",
    "\n",
    "select round(((tip_amount/total_amount * 100)),0) as tip_to_total_percent , count(*) as counts\n",
    "from nytaxi_view \n",
    "  group by round(((tip_amount/total_amount * 100)),0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Matplotlib + numpy initialization and imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#display(tip_ratio_pd)\n",
    "plt.bar(tip_ratio_pd['tip_to_total_percent'],tip_ratio_pd['counts'],width=0.8)\n",
    "plt.title('Tip Ratio vs Number of Rides');\n",
    "plt.xlabel('Tip Ratio')\n",
    "plt.ylabel('Number of Rides')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation : Most riders tend to tip 5% to 30% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on observation above we will only select samples with tip_ratio (tip/total) that are between 5% and 30% as outliers can skew the model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyctaxidf_filtered = spark.sql(\"select *\\\n",
    "                      from nytaxi_view\\\n",
    "                      where round(((tip_amount/total_amount * 100)),0) between 5 and 30 \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Perform feature engineering by converting Categorical features to Binary Vectors using OneHotEncoding and then assemble features in (label,feature) Vector\n",
    "\n",
    "Refer to https://spark.apache.org/docs/2.1.0/ml-features.html for complete set of feature extraction utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickupdowIndexer = StringIndexer(inputCol='pickup_dow_str',outputCol='pickup_dayofweek').setHandleInvalid(\"keep\")\n",
    "dropoffdowIndexer = StringIndexer(inputCol='dropoff_dow_str',outputCol='dropoff_dayofweek').setHandleInvalid(\"keep\")\n",
    "\n",
    "pickupdayEncoder = OneHotEncoder(inputCol='pickup_dayofweek',outputCol='pickupdowVec')\n",
    "dropoffdayEncoder = OneHotEncoder(inputCol='dropoff_dayofweek',outputCol='dropoffdowVec')\n",
    "\n",
    "pickuphourEncoder = OneHotEncoder(inputCol='pickup_hr',outputCol='pickup_hrVec')\n",
    "dropoffhourEncoder = OneHotEncoder(inputCol='dropoff_hr',outputCol='dropoff_hrVec')\n",
    "\n",
    "pu_locationidEncoder = OneHotEncoder(inputCol='pu_locationid',outputCol='pu_locationidVec')\n",
    "do_locationidEncoder = OneHotEncoder(inputCol='do_locationid',outputCol='do_locationidVec')\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['trip_distance','total_amount','pickupdowVec',\\\n",
    "                                       'dropoffdowVec','pickup_hrVec','dropoff_hrVec',\\\n",
    "                                       'pu_locationidVec','do_locationidVec'],outputCol='features')\n",
    "\n",
    "pipeline = Pipeline(stages=[pickupdowIndexer,dropoffdowIndexer,pickupdayEncoder,\\\n",
    "                            dropoffdayEncoder,pickuphourEncoder,dropoffhourEncoder,\\\n",
    "                            pu_locationidEncoder,do_locationidEncoder,assembler])\n",
    "\n",
    "model = pipeline.fit(nyctaxidf_filtered)\n",
    "\n",
    "transformed_nyctaxidf = model.transform(nyctaxidf_filtered)\n",
    "\n",
    "transformed_rdd = transformed_nyctaxidf.rdd.map(lambda x:(x.tip_amount,x.features))\n",
    "\n",
    "transformed_2_nyctaxidf = transformed_rdd.toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into training and test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on observation above we will only select samples with tip_ratio (tip/total) that are between 5% and 30% as outliers can skew the model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainDF,testDF) = transformed_2_nyctaxidf.toDF('label','features').randomSplit([.90,.10])   \n",
    "\n",
    "print(\"Number of training samples: {}\".format(trainDF.count()))\n",
    "print(\"Number of test samples: {}\".format(testDF.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Hosting XGBoost Model\n",
    "Now we create an XGBoostSageMakerEstimator, which uses the XGBoost Amazon SageMaker Algorithm to train on our input data, and uses the XGBoost Amazon SageMaker model image to host our model.\n",
    "\n",
    "The following cell initializes XGBoostSageMakerEstimator and set hyperparameters. Refer to https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html for more information on hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sagemaker_pyspark import IAMRole, S3DataPath\n",
    "from sagemaker_pyspark.algorithms import XGBoostSageMakerEstimator\n",
    "from sagemaker_pyspark.transformation import serializers\n",
    "\n",
    "xgboost_estimator = XGBoostSageMakerEstimator(\n",
    "    sagemakerRole=IAMRole(execution_role),\n",
    "    trainingInstanceType='ml.m5.large',\n",
    "    trainingInstanceCount=1,\n",
    "    endpointInstanceType='ml.m4.xlarge',\n",
    "    endpointInitialInstanceCount=1,\n",
    "    trainingInstanceVolumeSizeInGB=20\n",
    " )\n",
    "\n",
    "\n",
    "xgboost_estimator.setEta(0.2)\n",
    "xgboost_estimator.setGamma(4)\n",
    "xgboost_estimator.setMinChildWeight(6)\n",
    "xgboost_estimator.setSilent(0)\n",
    "xgboost_estimator.setObjective(\"reg:linear\")\n",
    "xgboost_estimator.setNumRound(50)\n",
    "xgboost_estimator.setEvalMetric(\"rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling fit() on this estimator will train our model on Amazon SageMaker, and then create an Amazon SageMaker Endpoint to host our model.\n",
    "\n",
    "We can then use the SageMakerModel returned by this call to fit() to transform Dataframes using our hosted model.\n",
    "\n",
    "The following cell runs a training job and creates an endpoint to host the resulting model, so this cell can take up to **twenty minutes to complete**.\n",
    "\n",
    "**After running Cell below, while waiting for SageMaker Training to finish, in a separate browser tab, using AWS console, navigate to SageMaker -> Training Jobs to observe job statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "model = xgboost_estimator.fit(trainDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Predictions\n",
    "\n",
    "Now we user the test Dataframe to call SageMaker Endpoint using transform() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformedData = model.transform(testDF)\n",
    "\n",
    "transformedData.select('label','prediction').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Spark Temp View of Prediction results to plot graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.dropTempView('predict_view')\n",
    "transformedData.createTempView(\"predict_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump SQL output to pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql -q -o predict_pd\n",
    "\n",
    "select monotonically_increasing_id() as id ,label, prediction from predict_view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the graph of Actuals vs Prediction of Tips to understand how well our model is doing\n",
    "\n",
    "**Disclaimer: The prediction accuracy may not be good as we have trained with 50 iterations but accuracy can be further greatly improved using hyperparameter tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Matplotlib + numpy initialization and imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.plot(predict_pd['id'],predict_pd['label'],'r--',predict_pd['id'],predict_pd['prediction'],'b--')\n",
    "plt.title('Compare Predictions vs Actuals');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean-up\n",
    "\n",
    "Since we don't need to make any more inferences, now we delete the resources (endpoints, models, configurations, etc):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the resources\n",
    "from sagemaker_pyspark import SageMakerResourceCleanup\n",
    "\n",
    "def cleanUp(model):\n",
    "    resource_cleanup = SageMakerResourceCleanup(model.sagemakerClient)\n",
    "    resource_cleanup.deleteResources(model.getCreatedResources())\n",
    "\n",
    "# Don't forget to include any models or pipeline models that you created in the notebook\n",
    "models = [model]\n",
    "\n",
    "# Delete regular SageMakerModels\n",
    "for m in models:\n",
    "    cleanUp(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.now()\n",
    "\n",
    "print(\"Total time to execute notebook:{} mins\".format((end_time-start_time).total_seconds()/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}