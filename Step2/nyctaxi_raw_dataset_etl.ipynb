{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an optimized NYC Taxi trips dataset\n",
    "\n",
    "\n",
    "### 1. Getting started\n",
    "\n",
    "We will write a script that applies the following transformations:\n",
    "\n",
    "1. Join NYC Taxi trips 'facts table' **yellow** with look-up tables (denormalization)\n",
    "2. Filter out (drop) unnecessary columns, rename existing columns, and create new columns\n",
    "3. Partition the dataset\n",
    "4. Convert the dataset to a columnar format (parquet)\n",
    "\n",
    "Let's begin by running some boilerplate to import AWS Glue and PySpark classes and functions we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "#AWS Glue imports\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "\n",
    "# Spark imports\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# PLEASE SET THIS VARIABLE to your own S3 bucket name. We'll store output dataset in it.\n",
    "your_bucket_name = '<your bucket name here>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load NYC Taxi dataset tables as DynamicFrames\n",
    "\n",
    "Next, we'll set up a single `GlueContext` and then load the NYC taxi dataset tables into DynamicFrames to apply our transformations. We'll print the schemas for extra verification before we perform further operations. These tables are stored as CSV files in Amazon S3, and cataloged in AWS Glue Data Catalog.\n",
    "\n",
    "The tables we'll load are:\n",
    "* **Yellow** taxi table. This is the biggest table of all, with millions of rows.\n",
    "* **Taxi Zone** table. This is a look-up table with 265 rows.\n",
    "* **Payment Type** table. Very small look-up table with 6 rows.\n",
    "* **Rate Code** table. Also a very small look-up table with 6 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up an AWS Glue context\n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "\n",
    "# Read our primary Yellow taxi table. This is the biggest table of all with millions of rows.\n",
    "yellow_dyf = glueContext.create_dynamic_frame.from_catalog(database=\"nyctaxi\", table_name=\"yellow\")\n",
    "yellow_dyf.printSchema()\n",
    "\n",
    "# Read the look-up tables\n",
    "taxi_zone_dyf = glueContext.create_dynamic_frame.from_catalog(database=\"nyctaxi\", table_name=\"taxizone\")\n",
    "taxi_zone_dyf.printSchema()\n",
    "\n",
    "payment_type_dyf = glueContext.create_dynamic_frame.from_catalog(database=\"nyctaxi\", table_name=\"paymenttype\")\n",
    "payment_type_dyf.printSchema()\n",
    "\n",
    "rate_code_dyf = glueContext.create_dynamic_frame.from_catalog(database=\"nyctaxi\", table_name=\"ratecode\")\n",
    "rate_code_dyf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Apply Mapping transformations and convert to Spark DataFrames\n",
    "\n",
    "This is where we write and test out ETL code until we achieve the results we're looking for.\n",
    "\n",
    "In the ETL code below, we use AWS Glue's ApplyMapping transformation on the look-up tables' DataFrames to map from existing look-up tables column names and data types to new names and data types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename look-up columns and change data types as needed. Convert into Spark DataFrames.\n",
    "\n",
    "pu_taxi_zone_df = taxi_zone_dyf.apply_mapping([\n",
    "    (\"locationid\", \"bigint\", \"pu_locationid\", \"bigint\"), \n",
    "    (\"borough\", \"string\", \"pu_borough\", \"string\"), \n",
    "    (\"zone\", \"string\", \"pu_zone\", \"string\"),\n",
    "    (\"service_zone\", \"string\", \"pu_service_zone\", \"string\")]).toDF()\n",
    "\n",
    "do_taxi_zone_df = taxi_zone_dyf.apply_mapping([\n",
    "    (\"locationid\", \"bigint\", \"do_locationid\", \"bigint\"), \n",
    "    (\"borough\", \"string\", \"do_borough\", \"string\"), \n",
    "    (\"zone\", \"string\", \"do_zone\", \"string\"),\n",
    "    (\"service_zone\", \"string\", \"do_service_zone\", \"string\")]).toDF()\n",
    "\n",
    "payment_type_df = payment_type_dyf.apply_mapping([\n",
    "    (\"id\", \"bigint\", \"payment_type_id\", \"bigint\"), \n",
    "    (\"name\", \"string\", \"payment_type_name\", \"string\")]).toDF()\n",
    "    \n",
    "rate_code_df = payment_type_dyf.apply_mapping([\n",
    "    (\"id\", \"bigint\", \"ratecode_id\", \"bigint\"), \n",
    "    (\"name\", \"string\", \"ratecode_name\", \"string\")]).toDF()\n",
    "\n",
    "# Convert into Spark DataFrames\n",
    "yellow_df = yellow_dyf.toDF()\n",
    "taxi_zone_df = taxi_zone_dyf.toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Repartition and cache() Yellow table\n",
    "\n",
    "Original number of partitions of Yellow table when loaded is 4. We increase that to 40 partitions (4 vCPUs * 10 DPUs = 40 partitions) to better leverage Spark parallelism on the Glue development endpoint.\n",
    "We also cache these partitions in memory to reduce execution time of subsequent Spark actions on the Yellow table\n",
    "We use repartition() since we're increasing the number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( 'Yellow table parts: {}'.format(yellow_df.rdd.getNumPartitions()) )\n",
    "yellow_df = yellow_df.repartition(4 * 10).cache() # 40 partitions to make use of Glue dev endpoint's 10 DPUs\n",
    "print( 'Repartitioned Yellow table parts: {}'.format(yellow_df.rdd.getNumPartitions()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Joining and Filtering \n",
    "\n",
    "Next, we convert Yellow taxi and look-ups DynamicFrames into Spark DataFrames. Then, we perform the following transformations.\n",
    "\n",
    "1. Create a denormalized table by joining our Yellow taxi table with look-up tables\n",
    "2. Create new pick-up (PU) & drop-off (DO) timestamp columns by casting string values from old PU and DO cols.\n",
    "3. Create dedicated partition columns *pick-up year* and *month*, because we anticipate reporting queries to filter on those two values.\n",
    "3. Drop unneeded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join with look-ups, add new cols, sort by year and month, then drop unnecessary cols\n",
    "yellow_opt_df = (yellow_df\n",
    "             .join(broadcast(pu_taxi_zone_df), yellow_df[\"pulocationid\"] == pu_taxi_zone_df[\"pu_locationid\"])\n",
    "             .join(broadcast(do_taxi_zone_df), yellow_df[\"dolocationid\"] == do_taxi_zone_df[\"do_locationid\"])\n",
    "             .join(broadcast(payment_type_df), yellow_df[\"payment_type\"] == payment_type_df[\"payment_type_id\"])\n",
    "             .join(broadcast(rate_code_df), yellow_df[\"ratecodeid\"] == rate_code_df[\"ratecode_id\"])\n",
    "             .withColumn(\"pu_datetime\", to_timestamp(col(\"tpep_pickup_datetime\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "             .withColumn(\"do_datetime\", to_timestamp(col(\"tpep_dropoff_datetime\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "             .withColumn(\"pu_year\", year(\"pu_datetime\"))\n",
    "             .withColumn(\"pu_month\", month(\"pu_datetime\"))\n",
    "             .withColumn(\"pu_day\", dayofmonth(\"pu_datetime\"))\n",
    "             .orderBy(\"pu_year\", \"pu_month\", \"pu_day\")\n",
    "             .drop(\"store_and_fwd_flag\",\n",
    "                   \"pulocationid\",\n",
    "                   \"dolocationid\",\n",
    "                   \"payment_type_id\",\n",
    "                   \"ratecodeid\",\n",
    "                   \"tpep_pickup_datetime\",\n",
    "                   \"tpep_dropoff_datetime\"\n",
    "                  )\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Repartition the data in Spark\n",
    "\n",
    "The previous transformations on Yellow result in 60 partitions.\n",
    "You can try to repartition or coalesce the Yellow dataframe and observe the effect on notebook execution time in addition to output files number and sizes.\n",
    "\n",
    "This ultimately affects query execution time and data scanned in Amazon Athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_opt_df = yellow_opt_df.coalesce(40)\n",
    "yellow_opt_df = yellow_opt_df.repartition(\"pu_year\", \"pu_month\", \"pu_day\")\n",
    "print( 'OPTIMIZED Yellow table parts: {}'.format(yellow_opt_df.rdd.getNumPartitions()) )\n",
    "\n",
    "# Use Spark's explain() to understand its physical plan to apply our transformations.\n",
    "# In this case, explain() was used to help optimize the Join process.\n",
    "# ####\n",
    "yellow_opt_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Optimized Write to S3\n",
    "\n",
    "Finally, we physically partition the output data in Amazon S3 into Hive-style partitions by *pick-up year* and *month* and convert the data into Parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parquet's Row Group size: 24, 32, and 64MB are reasonable values.\n",
    "blockSize = 1024 * 1024 * 32      #32MB\n",
    "\n",
    "(yellow_opt_df\n",
    " .write\n",
    " .mode(\"overwrite\")\n",
    " .format(\"parquet\")\n",
    " .option(\"parquet.block.size\", blockSize)\n",
    " .partitionBy(\"pu_year\", \"pu_month\")\n",
    " .save(\"s3://{}/data/staging/nyctaxi/yellow_opt/\".format(your_bucket_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}